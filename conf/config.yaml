defaults:
  - dset: AEC_echo_gen
  - hydra/job_logging: colorlog
  - hydra/hydra_logging: colorlog

# Dataset related
sample_rate: 16000
segment: 4
stride: 1    # in seconds, how much to stride between training examples
pad: true   # if training sample is too short, pad it
cv_maxlen: 8
validfull: 1   # use entire samples at valid

# Logging and printing, and does not impact training
num_prints: 5
device: cuda
num_workers: 4
verbose: 0
show: 0   # just show the model and its size and exit

# Checkpointing, by default automatically load last checkpoint
checkpoint: True
continue_from: ''  #'sep_echogen_echocat_0.9near_far_inter_sisnr' #'baseline_echogen_echocat_0.99near_inter' #'exp_baseline_AEC_dataset' # Only pass the name of the exp, like `exp_dset=wham`
                  # this arg is ignored for the naming of the exp!
continue_best: True
restart: False # Ignore existing checkpoints
checkpoint_file: checkpoint.th
history_file: history.json
samples_dir: samples
finetune: True

# Other stuff
seed: 1024
dummy:  # use this if you want twice the same exp, with a name

# Evaluation stuff
pesq: True # compute pesq?
eval_every: 10
keep_last: 0

# Optimization related
optim: adam
lr: 5e-4
beta2: 0.999
stft_loss: True
stft_sc_factor: .5
stft_mag_factor: .5
epochs: 200
batch_size: 4
max_norm: 5
# learning rate scheduling
lr_sched: plateau # can be either step or plateau
step: 
  step_size: 5
  gamma: 0.9
plateau:
  factor: 0.9
  patience: 3

# Models
model: echo_gen # either demucs or dwave
swave:
  N: 128
  L: 8
  H: 128
  R: 3
  C: 2
  input_normalize: False

# Experiment launching, distributed
ddp: True
ddp_backend: nccl
rendezvous_file: ./rendezvous

# Internal config, don't set manually
rank:
world_size:

# Hydra config
hydra:
  run:
    dir: ./outputs/sep_echogen_echocat_0.9near_far2_inter_si_snr_modified #${hydra.job.override_dirname}
  job:
    config:
      # configuration for the ${hydra.job.override_dirname} runtime variable
      override_dirname:
        kv_sep: '='
        item_sep: ','
        # Remove all paths, as the / in them would mess up things
        # Remove params that would not impact the training itself
        # Remove all slurm and submit params.
        # This is ugly I know...
        exclude_keys: [
          'hydra.job_logging.handles.file.filename',
          'dset.train', 'dset.valid', 'dset.test', 'dset.mix_json', 'dset.mix_dir',
          'num_prints', 'continue_from',
          'device', 'num_workers', 'print_freq', 'restart', 'verbose',
          'log', 'ddp', 'ddp_backend', 'rendezvous_file', 'rank', 'world_size']
  job_logging:
    handlers:
      file:
        class: logging.FileHandler
        mode: w
        formatter: colorlog
        filename: trainer.log
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr

  hydra_logging:
    handlers:
      console:
        class: logging.StreamHandler
        formatter: colorlog
        stream: ext://sys.stderr
